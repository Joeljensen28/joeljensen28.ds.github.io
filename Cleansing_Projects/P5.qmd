---
title: "Client Report - The War With Star Wars"
subtitle: "Course DS 250"
author: "Joel Jensen"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
#| label: libraries
#| include: false
import pandas as pd
import plotly.express as px
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
import seaborn as sn

pd.set_option('display.max_columns', None)
```


## Elevator pitch
_What does your taste in scifi-fantasy tell you about your success in life? Logically, it shouldn't tell you much, and that may be the case. But we can often predict some weird things with machines with greater accuracy than you would expect. For example, here we've plugged the results from a "Star Wars" survey into an MLM to see if it can predict if someone makes over $50k annually._

```{python}
#| label: project data
#| code-summary: Read and format project data
# Include and execute your code here
sw = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv', encoding='ISO-8859-1')
```

## How can you gain meaningful data from a survey?

__It's pretty difficult, but possible.__

_Survey data is notoriously messy and can take some time to clean up. Here, each question on the survey was its own column, and often the questions were long, droning sentences. So to start, the column names had to be shortened for ease of use._

```{python}
#| label: SW data
#| code-summary: Read and format data
#Choose words to extract from column titles to make it more succinct
pattern = 'ID|_|seen|any|films|Wars|consider|self|fan|rank|favorably|shot|first|familiar|Expanded|Universe|Star|Trek|Gender|Age|Income|Education|Location'
#Make the titles more suitable to pandas
sw.columns = sw.columns.str.replace(' ', '').str.findall(pattern).str.join('_')
#Create names to fill in blank column names
col_names = ['seen_epI',
'seen_epII',
'seen_epIII',
'seen_epIV',
'seen_epV',
'seen_epVI',
'rank_epI',
 'rank_epII',
 'rank_epIII',
 'rank_epIV',
 'rank_epV',
 'rank_epVI',
 'favorability_han',
 'favorability_luke',	
 'favorability_leia',	
 'favorability_anakin',	
 'favorability_obi_wan',	
 'favorability_palpatine',	
 'favorability_darth_vader',	
 'favorability_lando',	
 'favorability_boba_fett',	
 'favorability_c3p0',	
 'favorability_r2d2',	
 'favorability_jar_jar',	
 'favorability_padme',	
 'favorability_yoda']
#Insert col_names into indices 3-28 (where the column names are blank)
sw.columns = list(sw.columns[:3]) + col_names + list(sw.columns[29:])
#Drop the first row which contained extraneous details about the columns
sw = sw.iloc[1:]
#Reset the index for easier future programming
sw.reset_index(inplace=True)
#Drop column generated by .reset_index()
sw.drop(columns='index', inplace=True)

sw.head(3)

```

_Now it's much more succinct and easier to read._

## What can we learn about the Star Wars fanbase from this survey?

_The survey contains several questions pertaining to which movies the respondents have seenn (if any), how they feel about each character, and their favorite movies. So with this, we can learn a lot about how Star Wars fans feel about the franchise._

_Here we have a chart showing what percentage of respondents (who have seen at least one movie) have seen each movie._

```{python}
#| label: Charts
#| code-summary: Read and format data
#Filter data to only show those who have seen at least one film
seen = sw.iloc[:, 3:9].dropna(how='all')
#Write function to get the count of how many have seen each film
def get_count(column_name, df=sw):
    count = df[column_name].value_counts()
    value = df[column_name][0]
    count_of_value = count.get(value, 0)
    return count_of_value
#Make dataframe showing data
seen_movies = pd.DataFrame({'ep': ['The Phantom Menace', 'Attack of the Clones', 'Revenge of the Sith', 
                                      'A New Hope', 'The Empire Strikes Back', 'Return of the Jedi'],
                            'total_seen': [get_count('seen_epI'), get_count('seen_epII'), get_count('seen_epIII'),
                                           get_count('seen_epIV'), get_count('seen_epV'), get_count('seen_epVI')],
                            })
#Make new column for df that shows what percentage of people who have seen at least one movie have seen each movie
seen_movies['pct_seen'] = round((((seen_movies['total_seen'] / len(seen))) * 100), 0).astype(int)

px.bar(
    seen_movies,
    y='ep',
    x='pct_seen',
    category_orders={'ep': ['The Phantom Menace', 'Attack of the Clones', 'Revenge of the Sith', 
                                      'A New Hope', 'The Empire Strikes Back', 'Return of the Jedi']},
    text='pct_seen',
    title='Which \'Star Wars\' Movies Have You Seen?',
    labels={'ep': '',
            'pct_seen': ''}
).update_traces(
    textposition='outside',
    texttemplate='%{text}%'
).update_layout(
    {'plot_bgcolor': 'rgba(0, 0, 0, 0)'}
).update_xaxes(
    visible=False
)

```

_From this we can see that The Empire Strikes Back takes the cup for popularity, being the only movie to surpass 90%._

_This next one tells us what percentage of respondents think Han or Greedo shot first:_

```{python}
#| label: Charts Han vs Greedo
#| code-summary: Read and format data
#Make function that gets the count of multiple unique values
def get_count_unique():
    value1 = 'I don\'t understand this question'
    value2 = 'Han'
    value3 = 'Greedo'
    counts = sw['shot_first'].value_counts()
    counts_of_unsure, counts_of_han, counts_of_greedo = counts.get([value1, value2, value3], 0)
    return [counts_of_unsure, counts_of_han, counts_of_greedo]
#Put it in a df
shot_first = pd.DataFrame({
    'shot_first': ['I don\'t understand this question', 'Han', 'Greedo'],
    'sum_response': get_count_unique()
})
#Make new column that calculates what % of people gave each answer
shot_first['pct_response'] = round(((shot_first['sum_response'] / sum(shot_first['sum_response'])) * 100), 0).astype(int)

px.bar(
    shot_first,
    y='shot_first',
    x='pct_response',
    category_orders={'shot_first': ['Han', 'Greedo', 'I don\'t understand this question']},
    text='pct_response',
    title='Who Shot First?',
    labels={'shot_first': '',
            'pct_response': ''}
).update_traces(
    textposition='outside',
    texttemplate='%{text}%'
).update_layout(
    {'plot_bgcolor': 'rgba(0, 0, 0, 0)'}
).update_xaxes(
    visible=False
)

```

_Not surprising to know that most fans disagree with Lucas's choice to refilm that scene (and that over 1/3rd of respondents don't even know what's going on)._

## How does a machine process survey results?

__By having everything turned into numbers.__

_Machines don't do well with words or sentences, so we have to convert all the responses to a number that retains their meaning in some way. Here is an example:_

```{python}
#| label: MLDF
#| code-summary: Read and format data
#2a:
#Filter the dataset to the 835 who had seen one movie
som = sw.dropna(
    subset=['seen_epI', 'seen_epII', 'seen_epIII', 'seen_epIV', 'seen_epV', 'seen_epVI'],
    how='all'
)
#2b:
#Make a new column assigning a simple number to each age group
som['age_range'] = som['Age'].map(
    {'18-29': 23,
     '30-44': 37,
     '45-60': 52,
     '> 60': 68,
     np.nan: 45} #maybe change this to 45
)
#2c:
#Make a new column assigning a simple number to each education level
som['education_group'] = som['Education'].map(
    {'Less than high school degree': 9,
     'High school degree': 13,
     'Some college or Associate degree': 15,
     'Bachelor degree': 16,
     'Graduate degree': 18,
     np.nan: 14}
)
#2d:
#Make a new column assigning a simple number to each income category
som['income_category'] = som['Income'].map(
    {'$0 - $24,999': 1,
     '$25,000 - $49,999': 2,
     '$50,000 - $99,999': 3,
     '$100,000 - $149,999': 4,
     '$150,000+': 5,
     np.nan: 3}
)
#2e:
#Create the target column
som['income_greater_than_50k'] = np.where(
    (som['income_category'] > 3),
    1,
    0
)
#2f:
#Turn non-categorical columns into binary t/f statements
som['seen_any_films'] = np.where(
    (som['seen_any_films_Star_Wars'] == 'Yes'),
    1,
    0
)
som['fan'] = np.where(
    (som['consider_self_fan_Star_Wars'] == 'Yes'),
    1,
    0
)
som['seen_ep1'] = np.where(
    (som['seen_epI'] == 'Star Wars: Episode I  The Phantom Menace'),
    1,
    0
)
som['seen_ep2'] = np.where(
    (som['seen_epII'] == 'Star Wars: Episode II  Attack of the Clones'),
    1,
    0
)
som['seen_ep3'] = np.where(
    (som['seen_epIII'] == 'Star Wars: Episode III  Revenge of the Sith'),
    1,
    0
)
som['seen_ep4'] = np.where(
    (som['seen_epIV'] == 'Star Wars: Episode IV  A New Hope'),
    1,
    0
)
som['seen_ep5'] = np.where(
    (som['seen_epV'] == 'Star Wars: Episode V The Empire Strikes Back'),
    1,
    0
)
som['seen_ep6'] = np.where(
    (som['seen_epVI'] == 'Star Wars: Episode VI Return of the Jedi'),
    1,
    0
)
som['familiar_expanded'] = np.where(
    (som['familiar_Expanded_Universe'] == 'Yes'),
    1,
    0
)
som['fan_expanded'] = np.where(
    (som['consider_self_fan_Expanded_Universe'] == 'Yes'),
    1,
    0
)
som['fan_star_trek'] = np.where(
    (som['consider_self_fan_Star_Trek'] == 'Yes'),
    1,
    0
)
#Fill what few n/a's in the ranking columns with 0
som['rank_epI'].fillna(0, inplace=True)
som['rank_epIII'].fillna(0, inplace=True)
#Drop the old non-categorical columns containing strings
som.drop(columns=['seen_any_films_Star_Wars', 'consider_self_fan_Star_Wars',
       'seen_epI', 'seen_epII', 'seen_epIII', 'seen_epIV', 'seen_epV',
       'seen_epVI', 'familiar_Expanded_Universe',
       'consider_self_fan_Expanded_Universe', 'consider_self_fan_Star_Trek',
       'Age', 'Income', 'Education'], inplace=True)

#Facotorize the favorability columns
fact = som.iloc[:, 7:21].replace(
    {'Very favorably': 2,
     'Somewhat favorably': 1,
     'Neither favorably nor unfavorably (neutral)': 0,
     'Somewhat unfavorably': -1,
     'Very unfavorably': -2,
     'Unfamiliar (N/A)': 0,
     np.nan: 0}
)
#drop old columns
som.drop(columns=['favorability_han', 'favorability_luke', 'favorability_leia',
       'favorability_anakin', 'favorability_obi_wan', 'favorability_palpatine',
       'favorability_darth_vader', 'favorability_lando',
       'favorability_boba_fett', 'favorability_c3p0', 'favorability_r2d2',
       'favorability_jar_jar', 'favorability_padme', 'favorability_yoda'], inplace=True)

#Get dummies for the categorical columns
dummies = pd.get_dummies(som.filter(['shot_first', 'Gender', 'Location']))
#Improve names of dummy columns
dummies.columns = dummies.columns.str.replace(' ', '_').str.replace('N/A', '').str.replace('(', '').str.replace(')', '').str.replace('favorability', '').str.replace('neutral', '').str.strip('_').str.lower()
#Concatenate categorical and non-categorical dfs
ml = pd.concat([som, dummies, fact], axis=1)
#Drop old categorical columns
ml.drop(columns=['shot_first', 'Gender', 'Location'], inplace=True)
#Make sure all values in df are int
ml = ml.astype(int)

ml.filter(items=['id', 'favorability_han', 'gender_female', 'gender_male', 'income_greater_than_50k']).head()
```

_As you can see here, this dataset has only numbers. Columns like 'favorability_han' have been factorized and 'Gender' has been split between two columns and turned into binary true/false statements. We also have 'income_greater_than_50k' as a binary true/false, which is what we will be trying to predict._

## How well can we predict a respondent's income?

__Admittedly not as well as we had hoped, but still surprisingly well.__

_This model relies primarily upon features like 'age_range' and 'education_group', but receives lots of supplemental help from the questions about Star Wars as well. Here is the model's exact accuracy:_

```{python}
#| label: MLM
#| code-summary: Read and format data
#Make features df
features = ml.drop(columns=['income_greater_than_50k', 'income_category', 'ID', 'seen_any_films'])
x = features
#Set target
y = ml.income_greater_than_50k
#Split dataset for training and testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=777)
#Choose ML algorithm
lr_classifier = LogisticRegression()
#Fit the model
lr_classifier.fit(x_train, y_train)
#Make predictions
y_predict = lr_classifier.predict(x_test)
#Test accuracy
accuracy = metrics.accuracy_score(y_test, y_predict)
print(f'Accuracy: {accuracy}')
```

_As you can see, this model has roughly a 76% accuracy, and while most of that comes from the age and education features, a surprising amount of the accuracy comes from how people rated certain movies and characters._

## Conclusion

_In conclusion, we can't always predict things with amazing accuracy, but we can get somewhere. Maybe your feelings towards Jar Jar Binks has more to do with your success in life that you thought._